{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "ROOT_EXPORT = r\"data/2015-2025soybean_export\"\n",
    "ROOT_IMPORT = r\"data/2015-2025soybean_import\"\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2025-08-31\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_export_data(root_export):\n",
    "    export_dfs = []\n",
    "    for file_name in os.listdir(root_export):\n",
    "        if not (file_name.startswith(\"TradeData (\") and file_name.endswith(\".xlsx\")):\n",
    "            continue\n",
    "        df = pd.read_excel(os.path.join(root_export, file_name), sheet_name=0)\n",
    "        if \"flowCode\" in df.columns:\n",
    "            df = df[df[\"flowCode\"] == \"X\"]\n",
    "        df = df[df[\"reporterISO\"].isin([\"ARG\", \"BRA\", \"USA\"])]\n",
    "        if \"cmdCode\" in df.columns:\n",
    "            df = df[df[\"cmdCode\"].isin([120110, 120190])]\n",
    "        total_grp = df.groupby([\"refYear\", \"refMonth\", \"reporterISO\"]).agg(\n",
    "            Total_Export_Quantity=(\"netWgt\", \"sum\"),\n",
    "            Total_Export_Value=(\"fobvalue\", \"sum\"),\n",
    "        )\n",
    "        df_china = df[df[\"partnerISO\"] == \"CHN\"]\n",
    "        china_grp = df_china.groupby([\"refYear\", \"refMonth\", \"reporterISO\"]).agg(\n",
    "            Export_to_China_Quantity=(\"netWgt\", \"sum\"),\n",
    "            Export_to_China_Value=(\"fobvalue\", \"sum\"),\n",
    "        )\n",
    "        export_month = total_grp.join(china_grp, how=\"left\").reset_index()\n",
    "        export_month[\"Date\"] = pd.to_datetime(\n",
    "            export_month[\"refYear\"].astype(str) + \"-\" + export_month[\"refMonth\"].astype(str).str.zfill(2) + \"-01\"\n",
    "        )\n",
    "        export_dfs.append(export_month)\n",
    "    export_all = pd.concat(export_dfs, ignore_index=True)\n",
    "    export_pivot = export_all.pivot_table(\n",
    "        index=\"Date\",\n",
    "        columns=\"reporterISO\",\n",
    "        values=[\n",
    "            \"Total_Export_Quantity\",\n",
    "            \"Total_Export_Value\",\n",
    "            \"Export_to_China_Quantity\",\n",
    "            \"Export_to_China_Value\",\n",
    "        ],\n",
    "        aggfunc=\"sum\",\n",
    "    ).reset_index()\n",
    "    iso2prefix = {\"ARG\": \"AR\", \"BRA\": \"BR\", \"USA\": \"US\"}\n",
    "    def safe_get(df, key):\n",
    "        return df[key] if key in df.columns else np.nan\n",
    "    out = pd.DataFrame()\n",
    "    out[\"Date\"] = export_pivot[\"Date\"]\n",
    "    for iso, prefix in iso2prefix.items():\n",
    "        out[f\"{prefix}_Total_Export_Q\"] = safe_get(export_pivot, (\"Total_Export_Quantity\", iso))\n",
    "        out[f\"{prefix}_Total_Export_V\"] = safe_get(export_pivot, (\"Total_Export_Value\", iso))\n",
    "    for iso, prefix in iso2prefix.items():\n",
    "        out[f\"{prefix}_Export_China_Q\"] = safe_get(export_pivot, (\"Export_to_China_Quantity\", iso))\n",
    "        out[f\"{prefix}_Export_China_V\"] = safe_get(export_pivot, (\"Export_to_China_Value\", iso))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_customs_import_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    df = df[df[\"贸易伙伴名称\"].isin([\"美国\", \"巴西\", \"阿根廷\"])].copy()\n",
    "    df[\"数据年月\"] = df[\"数据年月\"].astype(str)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"数据年月\"].str[:4] + \"-\" + df[\"数据年月\"].str[4:6] + \"-01\")\n",
    "    df[\"Import_Quantity\"] = df[\"第一数量\"]\n",
    "    df[\"Import_Value\"] = df[\"美元\"]\n",
    "    country_map = {\"美国\": \"United States\", \"巴西\": \"Brazil\", \"阿根廷\": \"Argentina\"}\n",
    "    df[\"Country\"] = df[\"贸易伙伴名称\"].map(country_map)\n",
    "    return df[[\"Date\", \"Country\", \"Import_Quantity\", \"Import_Value\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_comtrade_import_data(root_import):\n",
    "    dfs = []\n",
    "    for file_name in os.listdir(root_import):\n",
    "        if not (file_name.startswith(\"TradeData (\") and file_name.endswith(\".xlsx\")):\n",
    "            continue\n",
    "        df = pd.read_excel(os.path.join(root_import, file_name), sheet_name=0)\n",
    "        if \"flowCode\" in df.columns:\n",
    "            df = df[df[\"flowCode\"] == \"M\"]\n",
    "        df = df[df[\"reporterISO\"] == \"CHN\"]\n",
    "        df = df[df[\"partnerISO\"].isin([\"USA\", \"BRA\", \"ARG\"])]\n",
    "        if \"cmdCode\" in df.columns:\n",
    "            df = df[df[\"cmdCode\"].isin([120110, 120190])]\n",
    "        grp = df.groupby([\"refYear\", \"refMonth\", \"partnerISO\"]).agg(\n",
    "            Import_Quantity=(\"netWgt\", \"sum\"),\n",
    "            Import_Value=(\"cifvalue\", \"sum\"),\n",
    "        ).reset_index()\n",
    "        grp[\"Date\"] = pd.to_datetime(grp[\"refYear\"].astype(str) + \"-\" + grp[\"refMonth\"].astype(str).str.zfill(2) + \"-01\")\n",
    "        iso2name = {\"USA\": \"United States\", \"BRA\": \"Brazil\", \"ARG\": \"Argentina\"}\n",
    "        grp[\"Country\"] = grp[\"partnerISO\"].map(iso2name)\n",
    "        dfs.append(grp[[\"Date\", \"Country\", \"Import_Quantity\", \"Import_Value\"]])\n",
    "    return pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_import_data(root_import):\n",
    "    customs_2015 = read_customs_import_data(os.path.join(root_import, \"2015 年大豆进口数据.xlsx\"))\n",
    "    customs_2025 = read_customs_import_data(os.path.join(root_import, \"2025 年大豆进口数据.xlsx\"))\n",
    "    comtrade_all = read_comtrade_import_data(root_import)\n",
    "    import_all = pd.concat([customs_2015, comtrade_all, customs_2025], ignore_index=True)\n",
    "    pivot = import_all.pivot_table(\n",
    "        index=\"Date\",\n",
    "        columns=\"Country\",\n",
    "        values=[\"Import_Quantity\", \"Import_Value\"],\n",
    "        aggfunc=\"sum\"\n",
    "    ).reset_index()\n",
    "    def safe_get(df, key):\n",
    "        return df[key] if key in df.columns else np.nan\n",
    "    out = pd.DataFrame()\n",
    "    out[\"Date\"] = pivot[\"Date\"]\n",
    "    countries = [(\"Argentina\", \"AR\"), (\"Brazil\", \"BR\"), (\"United States\", \"US\")]\n",
    "    for country, prefix in countries:\n",
    "        out[f\"{prefix}_Import_Q\"] = safe_get(pivot, (\"Import_Quantity\", country))\n",
    "    for country, prefix in countries:\n",
    "        out[f\"{prefix}_Import_V\"] = safe_get(pivot, (\"Import_Value\", country))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bilateral_data(export_data, import_data):\n",
    "    full_dates = pd.date_range(start=START_DATE, end=END_DATE, freq=\"MS\")\n",
    "    df = pd.DataFrame({\"Date\": full_dates})\n",
    "    df = pd.merge(df, export_data, on=\"Date\", how=\"left\")\n",
    "    df = pd.merge(df, import_data, on=\"Date\", how=\"left\")\n",
    "    def validate(row):\n",
    "        for exp_prefix, imp_prefix in [(\"US\", \"US\"), (\"AR\", \"AR\"), (\"BR\", \"BR\")]:\n",
    "            exp_q = row.get(f\"{exp_prefix}_Export_China_Q\", np.nan)\n",
    "            imp_q = row.get(f\"{imp_prefix}_Import_Q\", np.nan)\n",
    "            if pd.notna(exp_q) and pd.notna(imp_q):\n",
    "                error_rate = abs(exp_q - imp_q) / exp_q if exp_q != 0 else 0\n",
    "                row[f\"{exp_prefix}_Data_Error\"] = 1 if error_rate > 0.05 else 0\n",
    "            else:\n",
    "                row[f\"{exp_prefix}_Data_Error\"] = np.nan\n",
    "        return row\n",
    "    df = df.apply(validate, axis=1)\n",
    "    for exp_prefix, imp_prefix in [(\"US\", \"US\"), (\"AR\", \"AR\"), (\"BR\", \"BR\")]:\n",
    "        df[f\"{exp_prefix}_Bilateral_Q\"] = df[[f\"{exp_prefix}_Export_China_Q\", f\"{imp_prefix}_Import_Q\"]].mean(axis=1)\n",
    "        df[f\"{exp_prefix}_Bilateral_V\"] = df[[f\"{exp_prefix}_Export_China_V\", f\"{imp_prefix}_Import_V\"]].mean(axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supplement_china_us_tariff(bilateral_df):\n",
    "    policy_phases = [\n",
    "        {\"start\": \"2015-01-01\", \"end\": \"2018-06-30\", \"rate\": 0.03},\n",
    "        {\"start\": \"2018-07-01\", \"end\": \"2019-12-31\", \"rate\": 0.28},\n",
    "        {\"start\": \"2020-01-01\", \"end\": \"2024-12-31\", \"rate\": 0.03},\n",
    "        {\"start\": \"2025-01-01\", \"end\": \"2025-02-28\", \"rate\": 0.03},\n",
    "        {\"start\": \"2025-03-01\", \"end\": \"2025-03-31\", \"rate\": 0.13},\n",
    "        {\"start\": \"2025-04-01\", \"end\": \"2025-04-30\", \"rate\": 0.87},\n",
    "        {\"start\": \"2025-05-01\", \"end\": \"2025-08-31\", \"rate\": 0.13},\n",
    "    ]\n",
    "    bilateral_df = bilateral_df.copy()\n",
    "    bilateral_df[\"China_US_Tariff\"] = np.nan\n",
    "    for phase in policy_phases:\n",
    "        start_date = pd.to_datetime(phase[\"start\"])\n",
    "        end_date = pd.to_datetime(phase[\"end\"])\n",
    "        mask = (bilateral_df[\"Date\"] >= start_date) & (bilateral_df[\"Date\"] <= end_date)\n",
    "        bilateral_df.loc[mask, \"China_US_Tariff\"] = phase[\"rate\"]\n",
    "    return bilateral_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(bilateral_df):\n",
    "    bilateral_df = bilateral_df.copy()\n",
    "    bilateral_df[\"Month\"] = bilateral_df[\"Date\"].dt.month\n",
    "    for col in bilateral_df.columns:\n",
    "        if col in [\"Date\", \"US_Data_Error\", \"AR_Data_Error\", \"BR_Data_Error\", \"Month\"]:\n",
    "            continue\n",
    "        if not np.issubdtype(bilateral_df[col].dtype, np.number):\n",
    "            continue\n",
    "        bilateral_df[col] = bilateral_df.groupby(\"Month\")[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    bilateral_df = bilateral_df.interpolate(method=\"linear\", axis=0)\n",
    "    bilateral_df = bilateral_df.drop(\"Month\", axis=1)\n",
    "    return bilateral_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_features(clean_data):\n",
    "    feature_df = clean_data.copy()\n",
    "    for lag in range(1, 7):\n",
    "        for country in [\"US\", \"AR\", \"BR\"]:\n",
    "            feature_df[f\"{country}_Bilateral_Q_lag{lag}\"] = feature_df[f\"{country}_Bilateral_Q\"].shift(lag)\n",
    "    feature_df[\"Tariff_Change\"] = feature_df[\"China_US_Tariff\"].diff()\n",
    "    feature_df[\"Tariff_Change\"] = feature_df[\"Tariff_Change\"].fillna(0)\n",
    "    feature_df[\"Policy_Shock\"] = (feature_df[\"Tariff_Change\"].abs() > 0.05).astype(int)\n",
    "    feature_df[\"Total_Bilateral_Q\"] = feature_df[[\"US_Bilateral_Q\", \"AR_Bilateral_Q\", \"BR_Bilateral_Q\"]].sum(axis=1)\n",
    "    for country in [\"US\", \"AR\", \"BR\"]:\n",
    "        feature_df[f\"{country}_Export_Share\"] = feature_df[f\"{country}_Bilateral_Q\"] / feature_df[\"Total_Bilateral_Q\"]\n",
    "    for country in [\"US\", \"AR\", \"BR\"]:\n",
    "        price_col = f\"{country}_FOB_Price\"\n",
    "        feature_df[price_col] = feature_df[f\"{country}_Bilateral_V\"] / feature_df[f\"{country}_Bilateral_Q\"]\n",
    "        feature_df[price_col] = feature_df[price_col].replace([np.inf, -np.inf], np.nan)\n",
    "        feature_df[price_col] = feature_df[price_col].fillna(feature_df[price_col].mean())\n",
    "    feature_df = feature_df.dropna().reset_index(drop=True)\n",
    "    return feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_split_data(feature_data):\n",
    "    feature_data = feature_data.copy()\n",
    "    exclude_cols = [\n",
    "        \"Date\",\n",
    "        \"US_Data_Error\",\n",
    "        \"AR_Data_Error\",\n",
    "        \"BR_Data_Error\",\n",
    "        \"Policy_Shock\",\n",
    "        \"US_Export_Share\",\n",
    "        \"AR_Export_Share\",\n",
    "        \"BR_Export_Share\",\n",
    "    ]\n",
    "    scale_cols = [col for col in feature_data.columns if col not in exclude_cols and np.issubdtype(feature_data[col].dtype, np.number)]\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_data[scale_cols] = scaler.fit_transform(feature_data[scale_cols])\n",
    "    train_mask = feature_data[\"Date\"] <= \"2022-12-31\"\n",
    "    val_mask = (feature_data[\"Date\"] > \"2022-12-31\") & (feature_data[\"Date\"] <= \"2024-12-31\")\n",
    "    test_mask = feature_data[\"Date\"] > \"2024-12-31\"\n",
    "    train_df = feature_data[train_mask].reset_index(drop=True)\n",
    "    val_df = feature_data[val_mask].reset_index(drop=True)\n",
    "    test_df = feature_data[test_mask].reset_index(drop=True)\n",
    "    return train_df, val_df, test_df, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_data(train_data, val_data, test_data, scaler, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_data.to_csv(os.path.join(output_dir, \"train_data.csv\"), index=False)\n",
    "    val_data.to_csv(os.path.join(output_dir, \"val_data.csv\"), index=False)\n",
    "    test_data.to_csv(os.path.join(output_dir, \"test_data.csv\"), index=False)\n",
    "    joblib.dump(scaler, os.path.join(output_dir, \"minmax_scaler.pkl\"))\n",
    "    print(f\"预处理完成：{output_dir}\")\n",
    "    print(f\"训练集：{len(train_data)} 验证集：{len(val_data)} 测试集：{len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理完成：data/preprocessed_data\n",
      "训练集：84 验证集：24 测试集：8\n"
     ]
    }
   ],
   "source": [
    "export_data = read_export_data(ROOT_EXPORT)\n",
    "import_data = merge_import_data(ROOT_IMPORT)\n",
    "bilateral_data = merge_bilateral_data(export_data, import_data)\n",
    "bilateral_data = supplement_china_us_tariff(bilateral_data)\n",
    "clean_data = handle_missing_values(bilateral_data)\n",
    "feature_data = create_model_features(clean_data)\n",
    "train_data, val_data, test_data, scaler = scale_and_split_data(feature_data)\n",
    "save_preprocessed_data(train_data, val_data, test_data, scaler, output_dir=r\"data/preprocessed_data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
